{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "240fa22d",
   "metadata": {},
   "source": [
    "# Workshop 1 - 2D Collision Avoidance Using Reinforcement Learning\n",
    "\n",
    "<b>Project : MLPro - The integrative middleware framework for standardized machine learning</b> <br>\n",
    "<b>Package : [mlpro_int_sb3](https://mlpro-int-sb3.readthedocs.io/en/latest/)</b> <br>\n",
    "<b>Module : [howto_rl_env_003_train_agent_with_sb3_policy_on_2D_collision_avoidance_environment.py](https://github.com/fhswf/MLPro-Int-SB3/blob/main/test/howtos/environment/howto_rl_env_003_train_agent_with_sb3_policy_on_2D_collision_avoidance_environment.py)</b><br>\n",
    "<b>Version : 1.0.0</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd75d78",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Introduction of the Environment\n",
    "\n",
    "In this workshop, we aim to address a basic 2D collision avoidance problem using reinforcement learning. Details about the environment can be found at the following link: [2D Collision Detection Environment](https://mlpro.readthedocs.io/en/latest/content/03_machine_learning/mlpro_rl/sub/env/pool/2Dcollisiondetection.html).\n",
    "\n",
    "Initially, we will work with a setup involving 5 nodes, which include an initial node, a target node, and 2 controllable nodes. The obstacle remains static, while the target node can move among 4 different positions. The action space is constrained within the range of [-0.05, 0.05]. Before proceeding, you should be able to answer the following questions:\n",
    "1. What are the states?\n",
    "2. What is the action?\n",
    "3. What are the objectives of the RL training?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da108150",
   "metadata": {},
   "source": [
    "## Tasks Descriptions\n",
    "1. **Reward Shaping**: Developing a reward function to steer the RL agent towards desired behaviors more effectively by offering intermediate rewards or penalties.\n",
    "2. **Policy Selection**: Choosing from three different RL algorithms, namely PPO, A2C, and DDPG.\n",
    "3. **Run RL Training**: Execute the RL training setup in MLPro and analyze the results. If the goals are not met, consider revisiting and adjusting the previous two tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354400c4",
   "metadata": {},
   "source": [
    "## Executable Code\n",
    "\n",
    "### 0. Import the related modules\n",
    "The task involves importing the necessary libraries and packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db6e693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from mlpro.bf import Log\n",
    "from mlpro.bf.plot import DataPlotting\n",
    "from mlpro.bf.systems import State\n",
    "from mlpro.bf.ml import Model\n",
    "\n",
    "from mlpro.rl import *\n",
    "from mlpro.rl.pool.envs.collisionavoidance_2D import DynamicTrajectoryPlanner\n",
    "\n",
    "from stable_baselines3 import PPO, DDPG, A2C\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "\n",
    "from mlpro_int_sb3.wrappers import WrPolicySB32MLPro\n",
    "from mlpro_int_gymnasium.wrappers import WrEnvMLPro2GYM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5f0833",
   "metadata": {},
   "source": [
    "### 1. Design your own reward function\n",
    "Your first task is to create a reward function that assigns rewards and penalties to the agent for each iteration. You can design your reward function using the following three components:\n",
    "1. Number of Collide Points (`number_of_collide_points`): This parameter measures the number of collision points with the obstacle.\n",
    "2. Number of Collide Lines (`number_of_collide_lines`): This parameter tracks the number of collision lines with the obstacle.\n",
    "3. Distance of the Trajectory (`distance`): This parameter calculates the distance of the trajectory from the starting node to the target node, including visits to all controllable nodes.\n",
    "\n",
    "Then, the reward function should be stored in a variable named `total_rewards`.\n",
    "\n",
    "Here is an example of a reward function: If there is a collision with lines or points, a penalty of 10 points is given, along with a penalty based on the trajectory distance. Conversely, if there are no collisions, a reward of 10 points is granted. The reward function is designed as follows:\n",
    "```python\n",
    "if number_of_collide_lines != 0 or number_of_collide_points != 0:\n",
    "    total_rewards = -(10+distance)\n",
    "else:\n",
    "    total_rewards = 10\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a22fae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDynamicTrajectoryPlanner(DynamicTrajectoryPlanner):\n",
    "    \n",
    "    def _compute_reward(self, p_state_old:State, p_state_new:State) -> Reward:\n",
    "        \n",
    "        number_of_collide_points = 0\n",
    "        number_of_collide_lines = 0\n",
    "        for _ in self.collide_point_list:\n",
    "            number_of_collide_points += 1\n",
    "        for _ in self.collide_line_list:\n",
    "            number_of_collide_lines += 1\n",
    "\n",
    "        distance = self._calc_distance()\n",
    "        \n",
    "        # Insert your reward function in this section\n",
    "        if number_of_collide_lines != 0 or number_of_collide_points != 0:\n",
    "            total_rewards = -(10+distance)\n",
    "        else:\n",
    "            total_rewards = 10\n",
    "            \n",
    "        reward = Reward()\n",
    "        reward.set_overall_reward(total_rewards)\n",
    "        \n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c117d6bb",
   "metadata": {},
   "source": [
    "### 2. RL policy selection\n",
    "Your second task is to choose the most suitable RL algorithm. We provide three different algorithms from the Stable-Baselines3 (SB3) library, which is integrated with MLPro through a wrapper called MLPro-Int-SB3. The available algorithms are:\n",
    "1. [Advantage Actor-Critic (A2C)](https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html)\n",
    "2. [Proximal Policy Optimization (PPO)](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html)\n",
    "3. [Deep Deterministic Policy Gradient (DDPG)](https://stable-baselines3.readthedocs.io/en/master/modules/ddpg.html)\n",
    "\n",
    "We have provided default implementations for each algorithm, with standard parameter values already set. You can select an algorithm by placing the code for its initialization in the `ScenarioTrajectoryPlanning` Class. Below are the code snippets for each algorithm:\n",
    "1. **A2C**\n",
    "```python\n",
    "policy_kwargs = dict(activation_fn=torch.nn.Tanh,\n",
    "                      net_arch=dict(pi=[128, 128], vf=[128, 128]))\n",
    "policy_sb3 = A2C(\n",
    "    policy=\"MlpPolicy\",\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=100,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    env=None,\n",
    "    _init_setup_model=False,\n",
    "    device=\"cpu\",\n",
    "    seed=2)\n",
    "```\n",
    "2. **PPO**\n",
    "```python\n",
    "policy_kwargs = dict(activation_fn=torch.nn.Tanh,\n",
    "                      net_arch=dict(pi=[128, 128], vf=[128, 128]))\n",
    "policy_sb3 = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=100,\n",
    "    env=None,\n",
    "    _init_setup_model=False,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    device=\"cpu\",\n",
    "    seed=2)\n",
    "```\n",
    "3. **DDPG**\n",
    "```python\n",
    "action_space = WrEnvMLPro2GYM.recognize_space(self._env.get_action_space())\n",
    "n_actions = action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "policy_kwargs = dict(net_arch=dict(pi=[128, 128], qf=[128, 128]))\n",
    "policy_sb3 = DDPG(\n",
    "    policy=\"MlpPolicy\",\n",
    "    learning_rate=3e-4,\n",
    "    buffer_size=1000,\n",
    "    batch_size=128,\n",
    "    learning_starts=1001,\n",
    "    action_noise=action_noise,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    env=None,\n",
    "    _init_setup_model=False,\n",
    "    device=\"cpu\",\n",
    "    seed=3)\n",
    "```\n",
    "\n",
    "In the default version of these executable codes, the DDPG algorithm is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7081593f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScenarioTrajectoryPlanning(RLScenario):\n",
    "    C_NAME = 'Trajectory Planning'\n",
    "\n",
    "    def _setup(self, p_mode, p_ada: bool, p_visualize: bool, p_logging) -> Model:\n",
    "\n",
    "        self._env = MyDynamicTrajectoryPlanner(\n",
    "            p_visualize=p_visualize,\n",
    "            p_logging=p_logging,\n",
    "            p_num_point=4,\n",
    "            p_action_boundaries=[-0.05,0.05],\n",
    "            p_dt=0.01,\n",
    "            p_cycle_limit=500\n",
    "            )\n",
    "        \n",
    "        # Insert the initialization code for your selected algorithm in this section\n",
    "        action_space = WrEnvMLPro2GYM.recognize_space(self._env.get_action_space())\n",
    "        n_actions = action_space.shape[-1]\n",
    "        action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "        policy_kwargs = dict(net_arch=dict(pi=[128, 128], qf=[128, 128]))\n",
    "        policy_sb3 = DDPG(\n",
    "            policy=\"MlpPolicy\",\n",
    "            learning_rate=3e-4,\n",
    "            buffer_size=1000,\n",
    "            batch_size=128,\n",
    "            learning_starts=1001,\n",
    "            action_noise=action_noise,\n",
    "            policy_kwargs=policy_kwargs,\n",
    "            env=None,\n",
    "            _init_setup_model=False,\n",
    "            device=\"cpu\",\n",
    "            seed=3)\n",
    "\n",
    "        policy_wrapped = WrPolicySB32MLPro(\n",
    "            p_sb3_policy=policy_sb3,\n",
    "            p_cycle_limit=self._cycle_limit,\n",
    "            p_observation_space=self._env.get_state_space(),\n",
    "            p_action_space=self._env.get_action_space(),\n",
    "            p_ada=p_ada,\n",
    "            p_visualize=p_visualize,\n",
    "            p_logging=p_logging)\n",
    "\n",
    "        return Agent(\n",
    "            p_policy=policy_wrapped,\n",
    "            p_envmodel=None,\n",
    "            p_name='Smith',\n",
    "            p_ada=p_ada,\n",
    "            p_visualize=p_visualize,\n",
    "            p_logging=p_logging\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83549392",
   "metadata": {},
   "source": [
    "### 3. RL Training configuration\n",
    "Your next task is to initiate the RL training by executing the following code. You have the flexibility to adjust two key variables: `cycle_limit` and `cycles_per_epi_limit`. Currently, these are set to 10000 and 500, respectively. For the initial run, it is advisable to maintain these default values. Note that the environment's cycle limit is also 500, so ensure that the `cycles_per_epi_limit` does not exceed this value. Once the training begins, a plot will appear, allowing you to monitor the training progress in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3552d7d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m2025-09-18 10:24:40.445195  W  Training \"RL\": ------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m2025-09-18 10:24:40.445277  W  Training \"RL\": ------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m2025-09-18 10:24:40.445295  W  Training \"RL\": -- Training run 0 started...\u001b[0m\n",
      "\u001b[93m2025-09-18 10:24:40.445308  W  Training \"RL\": ------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m2025-09-18 10:24:40.445320  W  Training \"RL\": ------------------------------------------------------------------------------ \n",
      "\u001b[0m\n",
      "\u001b[93m2025-09-18 10:24:40.560893  W  Training \"RL\": ------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m2025-09-18 10:24:40.560969  W  Training \"RL\": -- Training episode 0 started...\u001b[0m\n",
      "\u001b[93m2025-09-18 10:24:40.560988  W  Training \"RL\": ------------------------------------------------------------------------------ \n",
      "\u001b[0m\n",
      "\u001b[91m2025-09-18 10:25:04.750443  E  RL-Scenario \"Trajectory Planning\": Process time 0:08:20 : Environment terminated\u001b[0m\n",
      "\u001b[93m2025-09-18 10:25:04.769818  W  Training \"RL\": Limit of 500 cycles per episode reached (Environment)\u001b[0m\n",
      "\u001b[93m2025-09-18 10:25:04.769883  W  Training \"RL\": ------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m2025-09-18 10:25:04.769899  W  Training \"RL\": -- Training episode 0 finished after 500 cycles\u001b[0m\n",
      "\u001b[93m2025-09-18 10:25:04.769913  W  Training \"RL\": -- Training cycles finished: 500\u001b[0m\n",
      "\u001b[93m2025-09-18 10:25:04.769927  W  Training \"RL\": ------------------------------------------------------------------------------ \n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "cannot remove artist",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m      6\u001b[39m plotting                = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m      8\u001b[39m training = RLTraining(\n\u001b[32m      9\u001b[39m     p_scenario_cls=ScenarioTrajectoryPlanning,\n\u001b[32m     10\u001b[39m     p_cycle_limit=cycle_limit,\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m     p_logging=logging\n\u001b[32m     15\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mtraining\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared_venv/lib/python3.12/site-packages/mlpro/bf/ml/basics.py:1469\u001b[39m, in \u001b[36mTraining.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1466\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._hpt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1467\u001b[39m     \u001b[38;5;66;03m# 1 Training without hyperparameter tuning\u001b[39;00m\n\u001b[32m   1468\u001b[39m     \u001b[38;5;28mself\u001b[39m.log(\u001b[38;5;28mself\u001b[39m.C_LOG_TYPE_I, \u001b[33m'\u001b[39m\u001b[33mTraining started (without hyperparameter tuning)\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1469\u001b[39m     \u001b[38;5;28mself\u001b[39m._results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1471\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1472\u001b[39m     \u001b[38;5;66;03m# 2 Training with hyperparameter tuning\u001b[39;00m\n\u001b[32m   1473\u001b[39m     \u001b[38;5;28mself\u001b[39m.log(\u001b[38;5;28mself\u001b[39m.C_LOG_TYPE_I, \u001b[33m'\u001b[39m\u001b[33mTraining started (with hyperparameter tuning)\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared_venv/lib/python3.12/site-packages/mlpro/bf/ml/basics.py:1486\u001b[39m, in \u001b[36mTraining._run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1484\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> TrainingResults:\n\u001b[32m   1485\u001b[39m     \u001b[38;5;28mself\u001b[39m._new_run = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1486\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_cycle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m: \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   1487\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_results()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared_venv/lib/python3.12/site-packages/mlpro/bf/ml/basics.py:1403\u001b[39m, in \u001b[36mTraining.run_cycle\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1401\u001b[39m \u001b[38;5;66;03m# 2 Run a single training cycle\u001b[39;00m\n\u001b[32m   1402\u001b[39m mode         = \u001b[38;5;28mself\u001b[39m._mode\n\u001b[32m-> \u001b[39m\u001b[32m1403\u001b[39m run_finished = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_cycle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode == \u001b[38;5;28mself\u001b[39m.C_MODE_TRAIN:\n\u001b[32m   1406\u001b[39m     \u001b[38;5;28mself\u001b[39m._results.num_cycles_train += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared_venv/lib/python3.12/site-packages/mlpro/rl/models_train.py:1148\u001b[39m, in \u001b[36mRLTraining._run_cycle\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1146\u001b[39m \u001b[38;5;66;03m# 1 Init next episode\u001b[39;00m\n\u001b[32m   1147\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._cycles_episode == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1148\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[38;5;66;03m# 2 Run a cycle\u001b[39;00m\n\u001b[32m   1151\u001b[39m success, error, timeout, limit, adapted, end_of_data = \u001b[38;5;28mself\u001b[39m._scenario.run_cycle()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared_venv/lib/python3.12/site-packages/mlpro/rl/models_train.py:926\u001b[39m, in \u001b[36mRLTraining._init_episode\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    923\u001b[39m         \u001b[38;5;28mself\u001b[39m.log(\u001b[38;5;28mself\u001b[39m.C_LOG_TYPE_W, Training.C_LOG_SEPARATOR, \u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m    925\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_scenario\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_results\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_eval_grp_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    928\u001b[39m     \u001b[38;5;28mself\u001b[39m.log(\u001b[38;5;28mself\u001b[39m.C_LOG_TYPE_W, Training.C_LOG_SEPARATOR)\n\u001b[32m    929\u001b[39m     \u001b[38;5;28mself\u001b[39m.log(\u001b[38;5;28mself\u001b[39m.C_LOG_TYPE_W, \u001b[33m'\u001b[39m\u001b[33m-- Training episode\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28mself\u001b[39m._results.num_episodes, \u001b[33m'\u001b[39m\u001b[33mstarted...\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared_venv/lib/python3.12/site-packages/mlpro/bf/ml/basics.py:905\u001b[39m, in \u001b[36mScenario.reset\u001b[39m\u001b[34m(self, p_seed)\u001b[39m\n\u001b[32m    893\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    894\u001b[39m \u001b[33;03mResets the scenario and especially the ML model inside. Internal random generators are seed \u001b[39;00m\n\u001b[32m    895\u001b[39m \u001b[33;03mwith the given value. Custom reset actions can be implemented in method _reset().\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    901\u001b[39m \n\u001b[32m    902\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    904\u001b[39m \u001b[38;5;66;03m# 1 Standard reset procedure including custom reset\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m905\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_seed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mp_seed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[38;5;66;03m# 2 Reset of internal ML model\u001b[39;00m\n\u001b[32m    909\u001b[39m \u001b[38;5;28mself\u001b[39m._model.set_random_seed(p_seed)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared_venv/lib/python3.12/site-packages/mlpro/bf/ops.py:263\u001b[39m, in \u001b[36mScenarioBase.reset\u001b[39m\u001b[34m(self, p_seed)\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[38;5;28mself\u001b[39m.log(\u001b[38;5;28mself\u001b[39m.C_LOG_TYPE_I, \u001b[33m'\u001b[39m\u001b[33mProcess time\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28mself\u001b[39m._timer.get_time(), \u001b[33m'\u001b[39m\u001b[33m: Scenario reset with seed\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28mstr\u001b[39m(p_seed))\n\u001b[32m    262\u001b[39m \u001b[38;5;66;03m# 2 Custom reset of further scenario-specific components\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_seed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[38;5;66;03m# 3 Timer reset\u001b[39;00m\n\u001b[32m    266\u001b[39m \u001b[38;5;28mself\u001b[39m._timer.reset()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared_venv/lib/python3.12/site-packages/mlpro/rl/models_train.py:475\u001b[39m, in \u001b[36mRLScenario._reset\u001b[39m\u001b[34m(self, p_seed)\u001b[39m\n\u001b[32m    465\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    466\u001b[39m \u001b[33;03mEnvironment and timer are reset. The random generators for environment and agent will\u001b[39;00m\n\u001b[32m    467\u001b[39m \u001b[33;03malso be reset. Optionally the agent's internal buffer data will be cleared, but\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    471\u001b[39m \u001b[33;03m    p_seed                  New seed for environment's and agent's random generator\u001b[39;00m\n\u001b[32m    472\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[38;5;66;03m# Reset environment\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_env\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_seed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._visualize:\n\u001b[32m    477\u001b[39m     \u001b[38;5;28mself\u001b[39m._env.init_plot()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared_venv/lib/python3.12/site-packages/mlpro/bf/systems/basics.py:1471\u001b[39m, in \u001b[36mSystem.reset\u001b[39m\u001b[34m(self, p_seed)\u001b[39m\n\u001b[32m   1468\u001b[39m         \u001b[38;5;28mself\u001b[39m._state.set_values(ob)\n\u001b[32m   1470\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1471\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_seed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1475\u001b[39m     \u001b[38;5;28mself\u001b[39m._state.set_initial(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared_venv/lib/python3.12/site-packages/mlpro/rl/pool/envs/collisionavoidance_2D.py:420\u001b[39m, in \u001b[36mDynamicTrajectoryPlanner._reset\u001b[39m\u001b[34m(self, p_seed)\u001b[39m\n\u001b[32m    417\u001b[39m \u001b[38;5;28mself\u001b[39m._calc_init_traj(\u001b[38;5;28mself\u001b[39m.num_traject_point)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._plot_avail:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mupdate_plot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    422\u001b[39m obs = \u001b[38;5;28mself\u001b[39m._get_obs()\n\u001b[32m    423\u001b[39m \u001b[38;5;28mself\u001b[39m._state = State(\u001b[38;5;28mself\u001b[39m._state_space)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared_venv/lib/python3.12/site-packages/mlpro/rl/pool/envs/collisionavoidance_2D.py:451\u001b[39m, in \u001b[36mDynamicTrajectoryPlanner.update_plot\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    449\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupdate_plot\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_clear_traj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    452\u001b[39m     \u001b[38;5;28mself\u001b[39m._draw_traj()\n\u001b[32m    453\u001b[39m     plt.pause(\u001b[38;5;28mself\u001b[39m.dt)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared_venv/lib/python3.12/site-packages/mlpro/rl/pool/envs/collisionavoidance_2D.py:242\u001b[39m, in \u001b[36mDynamicTrajectoryPlanner._clear_traj\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_clear_traj\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.start_plot \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.current_traject_plot \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.goal_plot:\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstart_plot\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mremove\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    243\u001b[39m         \u001b[38;5;28mself\u001b[39m.current_traject_plot.pop(\u001b[32m0\u001b[39m).remove()\n\u001b[32m    244\u001b[39m         \u001b[38;5;28mself\u001b[39m.goal_plot.pop(\u001b[32m0\u001b[39m).remove()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared_venv/lib/python3.12/site-packages/matplotlib/artist.py:256\u001b[39m, in \u001b[36mArtist.remove\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    253\u001b[39m         \u001b[38;5;28mself\u001b[39m._parent_figure = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mcannot remove artist\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNotImplementedError\u001b[39m: cannot remove artist"
     ]
    }
   ],
   "source": [
    "cycle_limit             = 10000\n",
    "cycles_per_epi_limit    = 500\n",
    "logging                 = Log.C_LOG_WE\n",
    "visualize               = True\n",
    "path                    = str(Path.home())\n",
    "plotting                = True\n",
    "\n",
    "training = RLTraining(\n",
    "    p_scenario_cls=ScenarioTrajectoryPlanning,\n",
    "    p_cycle_limit=cycle_limit,\n",
    "    p_cycles_per_epi_limit=cycles_per_epi_limit,\n",
    "    p_path=path,\n",
    "    p_visualize=visualize,\n",
    "    p_logging=logging\n",
    ")\n",
    "\n",
    "training.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d6788e",
   "metadata": {},
   "source": [
    "### 4. Plotting reward values\n",
    "You can execute the following lines of code to visualize the rewards generated by your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72bb15e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = training.get_results().ds_rewards\n",
    "data_printing = {mem.names[0]: [False],\n",
    "                 mem.names[1]: [False],\n",
    "                 mem.names[2]: [False],\n",
    "                 mem.names[3]: [False],\n",
    "                 mem.names[4]: [True, 0, -1]}\n",
    "mem_plot = DataPlotting(mem,\n",
    "                        p_showing=plotting,\n",
    "                        p_printing=data_printing,\n",
    "                        p_type=DataPlotting.C_PLOT_TYPE_EP,\n",
    "                        p_window=100)\n",
    "mem_plot.get_plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ec7e9b-4af3-4db8-8bad-b0c0cefae8cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
