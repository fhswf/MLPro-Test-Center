{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a91452a",
   "metadata": {},
   "source": [
    "# Workshop 2 - Path Planning of a UR5 Robot Using Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768fdd8e",
   "metadata": {},
   "source": [
    "## Introduction of the Environment\n",
    "\n",
    "In this workshop, our objective is to perform path planning for a UR5 robot using reinforcement learning. The UR5 environment, developed and visualized with MuJoCo, acts as a digital twin of the actual system. This environment is integrated with MLPro to utilize its RL capabilities.\n",
    "\n",
    "The environment is quite straightforward, as demonstrated in the video below:\n",
    "\n",
    "<img src=\"ur5_video.gif\" alt=\"SegmentLocal\" title=\"segment\" width=\"300\">\n",
    "\n",
    "In this setup, the UR5 robot is depicted with its end effector marked by a green node. The primary focus of this environment is on pick-and-place tasks. For this workshop, due to time constraints, we are concentrating solely on the pick operation. We have disregarded the target orientation and are only concerned with the position. The target picking position is set at (0, -0.33, 0.25) meters in x, y, and z coordinates, while the robot's home position is (1.54, -1.54, 1.54, -1.54, -1.54, 0.0) radians for the joint angles.\n",
    "\n",
    "To reduce training time, we have limited the exploration area by introducing a virtual wall, represented by a transparent blue box. If the end effector collides with this wall, the environment is reset. Additionally, an allowance of ±0.05 meters around the target position is included. The maximum joint speed is restricted to 0.5 radians per second, indicated by a red transparent box. The task is considered complete when the end effector is within this allowance.\n",
    "\n",
    "Regarding actions, they represent the joint velocities of the UR5 robot. Given that the robot has six degrees of freedom, the action space is six-dimensional, with velocities ranging from ±0.05 radians per second."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf52b18",
   "metadata": {},
   "source": [
    "## Tasks Descriptions\n",
    "1. **Reward Shaping**: Developing a reward function to steer the RL agent towards desired behaviors more effectively by offering intermediate rewards or penalties.\n",
    "2. **State Information Selection**: The process of choosing which aspects of the environment's state to include in the agent's state representation to effectively learn and make decisions.\n",
    "3. **Run RL Training**: Execute the RL training setup in MLPro and analyze the results. If the goals are not met, consider revisiting and adjusting the previous two tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ba6737",
   "metadata": {},
   "source": [
    "## Executable Code\n",
    "\n",
    "### 0. Import the related modules\n",
    "The task involves importing the necessary libraries and packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e0dd477",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from mlpro.bf import Log\n",
    "from mlpro.bf.plot import DataPlotting\n",
    "from mlpro.bf.systems import State\n",
    "from mlpro.bf.ml import Model\n",
    "from mlpro.rl import *\n",
    "\n",
    "from stable_baselines3 import PPO, DDPG, A2C, HerReplayBuffer\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "\n",
    "from mlpro_int_sb3.wrappers import WrPolicySB32MLPro\n",
    "from mlpro_int_gymnasium.wrappers import WrEnvMLPro2GYM\n",
    "\n",
    "from environment import UR5_Mujoco\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bebf4eb",
   "metadata": {},
   "source": [
    "### 1. Design your own reward function\n",
    "Your first task is to create a reward function that assigns rewards and penalties to the agent for each iteration. You can design your reward function using the following three components:\n",
    "1. Current End Effector Position (`current_eef`): This parameter tracks the current position of the end effector in x, y, and z coordinates.\n",
    "2. Target End Effector Position (`self._target_eef`): This parameter defines the target position for the end effector in x, y, and z coordinates.\n",
    "3. Target Reached (`target_reached`): This parameter indicates whether the end effector has successfully reached the target position.\n",
    "4. Environment Broken (`env_broken`): This parameter signifies that the environment has been disrupted due to a collision with the virtual wall.\n",
    "5. Distance between the Current and Target End Effector Positions (`distance`): This parameter measures the distance between the current and target end effector positions.\n",
    "\n",
    "Then, the reward function should be stored in a variable named `total_rewards`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12640b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyUR5_Mujoco(UR5_Mujoco):\n",
    "    \n",
    "    def _compute_reward(self, p_state_old:State, p_state_new:State) -> Reward:\n",
    "        \n",
    "        states_old      = p_state_old.get_values()\n",
    "        states_new      = p_state_new.get_values()\n",
    "        \n",
    "        if self._target_orientation:\n",
    "            current_eef = states_new[12:19]\n",
    "        else:\n",
    "            current_eef = states_new[12:15]\n",
    "        \n",
    "        if p_state_new.get_success():\n",
    "            target_reached = True\n",
    "        else:\n",
    "            target_reached = False\n",
    "            \n",
    "        if p_state_new.get_broken():\n",
    "            env_broken = True\n",
    "        else:\n",
    "            env_broken = False\n",
    "            \n",
    "        distance = -(np.linalg.norm(self._target_eef-current_eef))\n",
    "            \n",
    "        # Insert your reward function in this section\n",
    "        total_rewards = 0\n",
    "            \n",
    "        reward = Reward()\n",
    "        reward.set_overall_reward(total_rewards)\n",
    "        \n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19e186e",
   "metadata": {},
   "source": [
    "### 2. State information selection\n",
    "\n",
    "Your second task is to decide which state information is relevant for the RL agent to use. The environment provides five different sets of state information:\n",
    "S1. Current joint angles of the robot\n",
    "S2. Current joint velocity of the robot\n",
    "S3. Current end effector position\n",
    "S4. Current end effector orientation\n",
    "S5. Current end effector velocity\n",
    "\n",
    "You can activate or deactivate each set of state information by setting the corresponding variable to `True` for activation or `False` for deactivation. Below is an example of how to activate or deactivate the state information:\n",
    "```python\n",
    "ss_joint_angles     = True # S1 is activated\n",
    "ss_joint_velocity   = True # S2 is activated\n",
    "ss_eef_position     = True # S3 is activated\n",
    "ss_eef_orientation  = True # S4 is activated\n",
    "ss_eef_velocity     = True # S5 is activated\n",
    "```\n",
    "\n",
    "In the default version of this code, the PPO algorithm is selected with its default parameters initialized. You are welcome to modify the parameters as needed. You can find the documentation for the embedded PPO from Stable-Baselines3: [here](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d6886b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Scenario_UR5_Mujoco(RLScenario):\n",
    "    C_NAME = 'UR5_Mujoco'\n",
    "\n",
    "    def _setup(self, p_mode, p_ada: bool, p_visualize: bool, p_logging) -> Model:\n",
    "        \n",
    "        # Select your state information in this section\n",
    "        ss_joint_angles     = True\n",
    "        ss_joint_velocity   = True\n",
    "        ss_eef_position     = True\n",
    "        ss_eef_orientation  = True\n",
    "        ss_eef_velocity     = True\n",
    "        \n",
    "        self._env = MyUR5_Mujoco(\n",
    "            p_allowance_eef_pose=0.05,\n",
    "            p_velocity_max=0.5,\n",
    "            p_logging=p_logging\n",
    "            )\n",
    "        \n",
    "        policy_kwargs = dict(activation_fn=torch.nn.Tanh,\n",
    "                              net_arch=dict(pi=[64, 64], vf=[64, 64]))\n",
    "        policy_sb3 = PPO(\n",
    "            policy=\"MlpPolicy\",\n",
    "            n_steps=2048,\n",
    "            learning_rate=0.001,\n",
    "            batch_size=64,\n",
    "            env=None,\n",
    "            _init_setup_model=False,\n",
    "            policy_kwargs=policy_kwargs,\n",
    "            device=\"cpu\",\n",
    "            seed=2)\n",
    "        \n",
    "        state_space         = self._env.get_state_space()\n",
    "        list_ss             = []\n",
    "        \n",
    "        if ss_joint_angles:\n",
    "            for ss in range(6):\n",
    "                list_ss.append(state_space.get_dim_ids()[ss])\n",
    "                \n",
    "        if ss_joint_velocity:\n",
    "            for ss in range(6):\n",
    "                list_ss.append(state_space.get_dim_ids()[ss+6])\n",
    "                \n",
    "        if ss_eef_position:\n",
    "            for ss in range(3):\n",
    "                list_ss.append(state_space.get_dim_ids()[ss+12])\n",
    "                \n",
    "        if ss_eef_orientation:\n",
    "            for ss in range(4):\n",
    "                list_ss.append(state_space.get_dim_ids()[ss+15])\n",
    "                \n",
    "        if ss_eef_velocity:\n",
    "            for ss in range(6):\n",
    "                list_ss.append(state_space.get_dim_ids()[ss+19])\n",
    "        \n",
    "        _sspace = state_space.spawn(list_ss)\n",
    "\n",
    "        policy_wrapped = WrPolicySB32MLPro(\n",
    "            p_sb3_policy=policy_sb3,\n",
    "            p_cycle_limit=self._cycle_limit,\n",
    "            p_observation_space=_sspace,\n",
    "            p_action_space=self._env.get_action_space(),\n",
    "            p_ada=p_ada,\n",
    "            p_visualize=p_visualize,\n",
    "            p_logging=p_logging)\n",
    "\n",
    "        return Agent(\n",
    "            p_policy=policy_wrapped,\n",
    "            p_envmodel=None,\n",
    "            p_name='Smith',\n",
    "            p_ada=p_ada,\n",
    "            p_visualize=p_visualize,\n",
    "            p_logging=p_logging)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d763ae8",
   "metadata": {},
   "source": [
    "### 3. RL Training configuration\n",
    "Your next task is to initiate the RL training by executing the following code. You have the flexibility to adjust two key variables: `cycle_limit` and `cycles_per_epi_limit`. Currently, these are set to 30000 and 1000, respectively. For the initial run, it is advisable to maintain these default values. Once the training begins, a graph will appear, allowing you to monitor the training progress in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8609d9c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ParamError",
     "evalue": "Par p_scenario_cls: class \"Scenario_UR5_Mujoco\" not compatible",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/GitHub/fhswf/MLPro/src/mlpro/bf/ml/basics.py:1323\u001b[39m, in \u001b[36mTraining.__init__\u001b[39m\u001b[34m(self, **p_kwargs)\u001b[39m\n\u001b[32m   1322\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1323\u001b[39m     \u001b[38;5;28mself\u001b[39m._scenario = \u001b[43mscenario_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43menv_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m   1324\u001b[39m \u001b[43m                                   \u001b[49m\u001b[43mp_ada\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[43m                                   \u001b[49m\u001b[43mp_cycle_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cycle_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1326\u001b[39m \u001b[43m                                   \u001b[49m\u001b[43mp_visualize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1327\u001b[39m \u001b[43m                                   \u001b[49m\u001b[43mp_logging\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1328\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/GitHub/fhswf/MLPro/src/mlpro/rl/models_train.py:342\u001b[39m, in \u001b[36mRLScenario.__init__\u001b[39m\u001b[34m(self, p_mode, p_ada, p_cycle_limit, p_visualize, p_logging)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28mself\u001b[39m._env : Environment = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mp_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mp_ada\u001b[49m\u001b[43m=\u001b[49m\u001b[43mp_ada\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mp_cycle_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mp_cycle_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mp_visualize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mp_visualize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mp_logging\u001b[49m\u001b[43m=\u001b[49m\u001b[43mp_logging\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/GitHub/fhswf/MLPro/src/mlpro/bf/ml/basics.py:816\u001b[39m, in \u001b[36mScenario.__init__\u001b[39m\u001b[34m(self, p_mode, p_ada, p_cycle_limit, p_auto_setup, p_visualize, p_logging)\u001b[39m\n\u001b[32m    814\u001b[39m \u001b[38;5;28mself\u001b[39m._model_filename = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m816\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mp_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    817\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mp_cycle_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mp_cycle_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    818\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mp_auto_setup\u001b[49m\u001b[43m=\u001b[49m\u001b[43mp_auto_setup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    819\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mp_visualize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mp_visualize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    820\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mp_logging\u001b[49m\u001b[43m=\u001b[49m\u001b[43mp_logging\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/GitHub/fhswf/MLPro/src/mlpro/bf/ops.py:165\u001b[39m, in \u001b[36mScenarioBase.__init__\u001b[39m\u001b[34m(self, p_mode, p_id, p_cycle_limit, p_auto_setup, p_visualize, p_logging, **p_kwargs)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;66;03m# 2 Optional automatic custom setup\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p_auto_setup: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/GitHub/fhswf/MLPro/src/mlpro/bf/ml/basics.py:831\u001b[39m, in \u001b[36mScenario.setup\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    830\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msetup\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m831\u001b[39m     \u001b[38;5;28mself\u001b[39m._model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_mode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m                               \u001b[49m\u001b[43mp_ada\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ada\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    833\u001b[39m \u001b[43m                               \u001b[49m\u001b[43mp_visualize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_visualization\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[43m                               \u001b[49m\u001b[43mp_logging\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_log_level\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    836\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mScenario_UR5_Mujoco._setup\u001b[39m\u001b[34m(self, p_mode, p_ada, p_visualize, p_logging)\u001b[39m\n\u001b[32m     11\u001b[39m ss_eef_velocity     = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28mself\u001b[39m._env = \u001b[43mMyUR5_Mujoco\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mp_allowance_eef_pose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mp_velocity_max\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mp_logging\u001b[49m\u001b[43m=\u001b[49m\u001b[43mp_logging\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m policy_kwargs = \u001b[38;5;28mdict\u001b[39m(activation_fn=torch.nn.Tanh,\n\u001b[32m     20\u001b[39m                       net_arch=\u001b[38;5;28mdict\u001b[39m(pi=[\u001b[32m64\u001b[39m, \u001b[32m64\u001b[39m], vf=[\u001b[32m64\u001b[39m, \u001b[32m64\u001b[39m]))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/GitHub/fhswf/MLPro-Test-Center/edu/notebooks/rl/Path planning UR5 robot/jupyter/environment.py:42\u001b[39m, in \u001b[36mUR5_Mujoco.__init__\u001b[39m\u001b[34m(self, p_allowance_eef_pose, p_allowance_eef_orient, p_target_orientation, p_env_boundaries, p_velocity_max, p_logging)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28mself\u001b[39m._state_space, \u001b[38;5;28mself\u001b[39m._action_space = \u001b[38;5;28mself\u001b[39m._setup_spaces()\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/GitHub/fhswf/MLPro/src/mlpro/bf/systems/basics.py:1471\u001b[39m, in \u001b[36mSystem.reset\u001b[39m\u001b[34m(self, p_seed)\u001b[39m\n\u001b[32m   1470\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1471\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_seed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/GitHub/fhswf/MLPro-Test-Center/edu/notebooks/rl/Path planning UR5 robot/jupyter/environment.py:203\u001b[39m, in \u001b[36mUR5_Mujoco._reset\u001b[39m\u001b[34m(self, p_seed)\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_reset\u001b[39m(\u001b[38;5;28mself\u001b[39m, p_seed=\u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    197\u001b[39m     \n\u001b[32m    198\u001b[39m     \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[32m    199\u001b[39m     \u001b[38;5;66;03m#     self._ori_env.close()\u001b[39;00m\n\u001b[32m    200\u001b[39m     \u001b[38;5;66;03m# except:\u001b[39;00m\n\u001b[32m    201\u001b[39m     \u001b[38;5;66;03m#     pass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ori_env\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    205\u001b[39m     obs = \u001b[38;5;28mself\u001b[39m._get_init_obs()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared_venv/lib/python3.12/site-packages/gymnasium/wrappers/order_enforcing.py:61\u001b[39m, in \u001b[36mOrderEnforcing.reset\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28mself\u001b[39m._has_reset = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared_venv/lib/python3.12/site-packages/gymnasium/wrappers/env_checker.py:55\u001b[39m, in \u001b[36mPassiveEnvChecker.reset\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m     54\u001b[39m     \u001b[38;5;28mself\u001b[39m.checked_reset = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_reset_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared_venv/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:215\u001b[39m, in \u001b[36menv_reset_passive_checker\u001b[39m\u001b[34m(env, **kwargs)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;66;03m# Checks the result of env.reset with kwargs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m result = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mtuple\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared_venv/lib/python3.12/site-packages/gymnasium/envs/mujoco/mujoco_env.py:129\u001b[39m, in \u001b[36mBaseMujocoEnv.reset\u001b[39m\u001b[34m(self, seed, options)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.render_mode == \u001b[33m\"\u001b[39m\u001b[33mhuman\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ob, {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared_venv/lib/python3.12/site-packages/gymnasium/envs/mujoco/mujoco_env.py:379\u001b[39m, in \u001b[36mMujocoEnv.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m379\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmujoco_renderer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrender_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcamera_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcamera_name\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared_venv/lib/python3.12/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py:673\u001b[39m, in \u001b[36mMujocoRenderer.render\u001b[39m\u001b[34m(self, render_mode, camera_id, camera_name)\u001b[39m\n\u001b[32m    672\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m render_mode == \u001b[33m\"\u001b[39m\u001b[33mhuman\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m673\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mviewer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared_venv/lib/python3.12/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py:413\u001b[39m, in \u001b[36mWindowViewer.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    412\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._loop_count > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m413\u001b[39m     \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28mself\u001b[39m._loop_count -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared_venv/lib/python3.12/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py:353\u001b[39m, in \u001b[36mWindowViewer.render.<locals>.update\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    351\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupdate\u001b[39m():\n\u001b[32m    352\u001b[39m     \u001b[38;5;66;03m# fill overlay items\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_overlay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    355\u001b[39m     render_start = time.time()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/shared_venv/lib/python3.12/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py:593\u001b[39m, in \u001b[36mWindowViewer._create_overlay\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    591\u001b[39m \u001b[38;5;28mself\u001b[39m.add_overlay(bottomleft, \u001b[33m\"\u001b[39m\u001b[33mFPS\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % (\u001b[32m1\u001b[39m / \u001b[38;5;28mself\u001b[39m._time_per_render, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m    592\u001b[39m \u001b[38;5;28mself\u001b[39m.add_overlay(\n\u001b[32m--> \u001b[39m\u001b[32m593\u001b[39m     bottomleft, \u001b[33m\"\u001b[39m\u001b[33mSolver iterations\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43msolver_iter\u001b[49m + \u001b[32m1\u001b[39m)\n\u001b[32m    594\u001b[39m )\n\u001b[32m    595\u001b[39m \u001b[38;5;28mself\u001b[39m.add_overlay(\n\u001b[32m    596\u001b[39m     bottomleft, \u001b[33m\"\u001b[39m\u001b[33mStep\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mround\u001b[39m(\u001b[38;5;28mself\u001b[39m.data.time / \u001b[38;5;28mself\u001b[39m.model.opt.timestep))\n\u001b[32m    597\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'mujoco._structs.MjData' object has no attribute 'solver_iter'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mParamError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m path                    = \u001b[38;5;28mstr\u001b[39m(Path.home())\n\u001b[32m      6\u001b[39m plotting                = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m training = \u001b[43mRLTraining\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mp_scenario_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mScenario_UR5_Mujoco\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mp_cycle_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcycle_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mp_cycles_per_epi_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcycles_per_epi_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mp_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mp_visualize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mp_logging\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m training.run()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/GitHub/fhswf/MLPro/src/mlpro/rl/models_train.py:725\u001b[39m, in \u001b[36mRLTraining.__init__\u001b[39m\u001b[34m(self, **p_kwargs)\u001b[39m\n\u001b[32m    722\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, **p_kwargs):\n\u001b[32m    723\u001b[39m \n\u001b[32m    724\u001b[39m     \u001b[38;5;66;03m# 1 Initialization of elementary training functionalities\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m725\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mp_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    727\u001b[39m     \u001b[38;5;66;03m# 2 Check and completion of RL-specific parameters\u001b[39;00m\n\u001b[32m    728\u001b[39m \n\u001b[32m    729\u001b[39m     \u001b[38;5;66;03m# 2.1 Optional parameter p_cycles_per_epi_limit\u001b[39;00m\n\u001b[32m    730\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/GitHub/fhswf/MLPro/src/mlpro/bf/ml/basics.py:1329\u001b[39m, in \u001b[36mTraining.__init__\u001b[39m\u001b[34m(self, **p_kwargs)\u001b[39m\n\u001b[32m   1323\u001b[39m     \u001b[38;5;28mself\u001b[39m._scenario = scenario_cls( p_mode=env_mode, \n\u001b[32m   1324\u001b[39m                                    p_ada=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   1325\u001b[39m                                    p_cycle_limit=\u001b[38;5;28mself\u001b[39m._cycle_limit,\n\u001b[32m   1326\u001b[39m                                    p_visualize=visualize,\n\u001b[32m   1327\u001b[39m                                    p_logging=logging )\n\u001b[32m   1328\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1329\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ParamError(\u001b[33m'\u001b[39m\u001b[33mPar p_scenario_cls: class \u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m + scenario_cls.\u001b[34m__name__\u001b[39m + \u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m not compatible\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mParamError\u001b[39m: Par p_scenario_cls: class \"Scenario_UR5_Mujoco\" not compatible"
     ]
    }
   ],
   "source": [
    "cycle_limit             = 30000\n",
    "cycles_per_epi_limit    = 1000\n",
    "logging                 = Log.C_LOG_WE\n",
    "visualize               = True\n",
    "path                    = str(Path.home())\n",
    "plotting                = True\n",
    "\n",
    "training = RLTraining(\n",
    "    p_scenario_cls=Scenario_UR5_Mujoco,\n",
    "    p_cycle_limit=cycle_limit,\n",
    "    p_cycles_per_epi_limit=cycles_per_epi_limit,\n",
    "    p_path=path,\n",
    "    p_visualize=visualize,\n",
    "    p_logging=logging\n",
    ")\n",
    "\n",
    "training.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00742266",
   "metadata": {},
   "source": [
    "### 4. Plotting reward values\n",
    "You can execute the following lines of code to visualize the rewards generated by your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6690157d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mem = training.get_results().ds_rewards\n",
    "data_printing = {mem.names[0]: [False],\n",
    "                 mem.names[1]: [False],\n",
    "                 mem.names[2]: [False],\n",
    "                 mem.names[3]: [False],\n",
    "                 mem.names[4]: [True, 0, -1]}\n",
    "mem_plot = DataPlotting(mem,\n",
    "                        p_showing=plotting,\n",
    "                        p_printing=data_printing,\n",
    "                        p_type=DataPlotting.C_PLOT_TYPE_EP_S,\n",
    "                        p_window=10)\n",
    "mem_plot.get_plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990429a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
