{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a91452a",
   "metadata": {},
   "source": [
    "# Workshop 2 - Path Planning of a UR5 Robot Using Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768fdd8e",
   "metadata": {},
   "source": [
    "## Introduction of the Environment\n",
    "\n",
    "In this workshop, our objective is to perform path planning for a UR5 robot using reinforcement learning. The UR5 environment, developed and visualized with MuJoCo, acts as a digital twin of the actual system. This environment is integrated with MLPro to utilize its RL capabilities.\n",
    "\n",
    "The environment is quite straightforward, as demonstrated in the video below:\n",
    "\n",
    "<img src=\"ur5_video.gif\" alt=\"SegmentLocal\" title=\"segment\" width=\"300\">\n",
    "\n",
    "In this setup, the UR5 robot is depicted with its end effector marked by a green node. The primary focus of this environment is on pick-and-place tasks. For this workshop, due to time constraints, we are concentrating solely on the pick operation. We have disregarded the target orientation and are only concerned with the position. The target picking position is set at (0, -0.33, 0.25) meters in x, y, and z coordinates, while the robot's home position is (1.54, -1.54, 1.54, -1.54, -1.54, 0.0) radians for the joint angles.\n",
    "\n",
    "To reduce training time, we have limited the exploration area by introducing a virtual wall, represented by a transparent blue box. If the end effector collides with this wall, the environment is reset. Additionally, an allowance of ±0.05 meters around the target position is included. The maximum joint speed is restricted to 0.5 radians per second, indicated by a red transparent box. The task is considered complete when the end effector is within this allowance.\n",
    "\n",
    "Regarding actions, they represent the joint velocities of the UR5 robot. Given that the robot has six degrees of freedom, the action space is six-dimensional, with velocities ranging from ±0.05 radians per second."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf52b18",
   "metadata": {},
   "source": [
    "## Tasks Descriptions\n",
    "1. **Reward Shaping**: Developing a reward function to steer the RL agent towards desired behaviors more effectively by offering intermediate rewards or penalties.\n",
    "2. **State Information Selection**: The process of choosing which aspects of the environment's state to include in the agent's state representation to effectively learn and make decisions.\n",
    "3. **Run RL Training**: Execute the RL training setup in MLPro and analyze the results. If the goals are not met, consider revisiting and adjusting the previous two tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ba6737",
   "metadata": {},
   "source": [
    "## Executable Code\n",
    "\n",
    "### 0. Import the related modules\n",
    "The task involves importing the necessary libraries and packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e0dd477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mlpro.bf.plot import DataPlotting\n",
    "from mlpro.rl import *\n",
    "from environment import UR5_Mujoco\n",
    "from stable_baselines3 import PPO, DDPG, A2C, HerReplayBuffer\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "from mlpro_int_sb3.wrappers import WrPolicySB32MLPro\n",
    "from mlpro_int_gymnasium.wrappers import WrEnvMLPro2GYM\n",
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bebf4eb",
   "metadata": {},
   "source": [
    "### 1. Design your own reward function\n",
    "Your first task is to create a reward function that assigns rewards and penalties to the agent for each iteration. You can design your reward function using the following three components:\n",
    "1. Current End Effector Position (`current_eef`): This parameter tracks the current position of the end effector in x, y, and z coordinates.\n",
    "2. Target End Effector Position (`self._target_eef`): This parameter defines the target position for the end effector in x, y, and z coordinates.\n",
    "3. Target Reached (`target_reached`): This parameter indicates whether the end effector has successfully reached the target position.\n",
    "4. Environment Broken (`env_broken`): This parameter signifies that the environment has been disrupted due to a collision with the virtual wall.\n",
    "5. Distance between the Current and Target End Effector Positions (`distance`): This parameter measures the distance between the current and target end effector positions.\n",
    "\n",
    "Then, the reward function should be stored in a variable named `total_rewards`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12640b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyUR5_Mujoco(UR5_Mujoco):\n",
    "    \n",
    "    def _compute_reward(self, p_state_old:State, p_state_new:State) -> Reward:\n",
    "        \n",
    "        states_old      = p_state_old.get_values()\n",
    "        states_new      = p_state_new.get_values()\n",
    "        \n",
    "        if self._target_orientation:\n",
    "            current_eef = states_new[12:19]\n",
    "        else:\n",
    "            current_eef = states_new[12:15]\n",
    "        \n",
    "        if p_state_new.get_success():\n",
    "            target_reached = True\n",
    "        else:\n",
    "            target_reached = False\n",
    "            \n",
    "        if p_state_new.get_broken():\n",
    "            env_broken = True\n",
    "        else:\n",
    "            env_broken = False\n",
    "            \n",
    "        distance = -(np.linalg.norm(self._target_eef-current_eef))\n",
    "            \n",
    "        # Insert your reward function in this section\n",
    "        total_rewards = 0\n",
    "            \n",
    "        reward = Reward()\n",
    "        reward.set_overall_reward(total_rewards)\n",
    "        \n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19e186e",
   "metadata": {},
   "source": [
    "### 2. State information selection\n",
    "\n",
    "Your second task is to decide which state information is relevant for the RL agent to use. The environment provides five different sets of state information:\n",
    "S1. Current joint angles of the robot\n",
    "S2. Current joint velocity of the robot\n",
    "S3. Current end effector position\n",
    "S4. Current end effector orientation\n",
    "S5. Current end effector velocity\n",
    "\n",
    "You can activate or deactivate each set of state information by setting the corresponding variable to `True` for activation or `False` for deactivation. Below is an example of how to activate or deactivate the state information:\n",
    "```python\n",
    "ss_joint_angles     = True # S1 is activated\n",
    "ss_joint_velocity   = True # S2 is activated\n",
    "ss_eef_position     = True # S3 is activated\n",
    "ss_eef_orientation  = True # S4 is activated\n",
    "ss_eef_velocity     = True # S5 is activated\n",
    "```\n",
    "\n",
    "In the default version of this code, the PPO algorithm is selected with its default parameters initialized. You are welcome to modify the parameters as needed. You can find the documentation for the embedded PPO from Stable-Baselines3: [here](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d6886b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Scenario_UR5_Mujoco(RLScenario):\n",
    "    C_NAME = 'UR5_Mujoco'\n",
    "\n",
    "    def _setup(self, p_mode, p_ada: bool, p_visualize: bool, p_logging) -> Model:\n",
    "        \n",
    "        # Select your state information in this section\n",
    "        ss_joint_angles     = True\n",
    "        ss_joint_velocity   = True\n",
    "        ss_eef_position     = True\n",
    "        ss_eef_orientation  = True\n",
    "        ss_eef_velocity     = True\n",
    "        \n",
    "        self._env = MyUR5_Mujoco(\n",
    "            p_allowance_eef_pose=0.05,\n",
    "            p_velocity_max=0.5,\n",
    "            p_logging=p_logging\n",
    "            )\n",
    "        \n",
    "        policy_kwargs = dict(activation_fn=torch.nn.Tanh,\n",
    "                              net_arch=dict(pi=[64, 64], vf=[64, 64]))\n",
    "        policy_sb3 = PPO(\n",
    "            policy=\"MlpPolicy\",\n",
    "            n_steps=2048,\n",
    "            learning_rate=0.001,\n",
    "            batch_size=64,\n",
    "            env=None,\n",
    "            _init_setup_model=False,\n",
    "            policy_kwargs=policy_kwargs,\n",
    "            device=\"cpu\",\n",
    "            seed=2)\n",
    "        \n",
    "        state_space         = self._env.get_state_space()\n",
    "        list_ss             = []\n",
    "        \n",
    "        if ss_joint_angles:\n",
    "            for ss in range(6):\n",
    "                list_ss.append(state_space.get_dim_ids()[ss])\n",
    "                \n",
    "        if ss_joint_velocity:\n",
    "            for ss in range(6):\n",
    "                list_ss.append(state_space.get_dim_ids()[ss+6])\n",
    "                \n",
    "        if ss_eef_position:\n",
    "            for ss in range(3):\n",
    "                list_ss.append(state_space.get_dim_ids()[ss+12])\n",
    "                \n",
    "        if ss_eef_orientation:\n",
    "            for ss in range(4):\n",
    "                list_ss.append(state_space.get_dim_ids()[ss+15])\n",
    "                \n",
    "        if ss_eef_velocity:\n",
    "            for ss in range(6):\n",
    "                list_ss.append(state_space.get_dim_ids()[ss+19])\n",
    "        \n",
    "        _sspace = state_space.spawn(list_ss)\n",
    "\n",
    "        policy_wrapped = WrPolicySB32MLPro(\n",
    "            p_sb3_policy=policy_sb3,\n",
    "            p_cycle_limit=self._cycle_limit,\n",
    "            p_observation_space=_sspace,\n",
    "            p_action_space=self._env.get_action_space(),\n",
    "            p_ada=p_ada,\n",
    "            p_visualize=p_visualize,\n",
    "            p_logging=p_logging)\n",
    "\n",
    "        return Agent(\n",
    "            p_policy=policy_wrapped,\n",
    "            p_envmodel=None,\n",
    "            p_name='Smith',\n",
    "            p_ada=p_ada,\n",
    "            p_visualize=p_visualize,\n",
    "            p_logging=p_logging)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d763ae8",
   "metadata": {},
   "source": [
    "### 3. RL Training configuration\n",
    "Your next task is to initiate the RL training by executing the following code. You have the flexibility to adjust two key variables: `cycle_limit` and `cycles_per_epi_limit`. Currently, these are set to 30000 and 1000, respectively. For the initial run, it is advisable to maintain these default values. Once the training begins, a graph will appear, allowing you to monitor the training progress in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8609d9c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yuwono\\.conda\\envs\\workshop\\lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.data to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.data` for environment variables or `env.get_wrapper_attr('data')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m2024-09-03  14:09:34.979919  W  Training \"RL\": ------------------------------------------------------------------------------ \u001b[0m\n",
      "\u001b[93m2024-09-03  14:09:34.980960  W  Training \"RL\": ------------------------------------------------------------------------------ \u001b[0m\n",
      "\u001b[93m2024-09-03  14:09:34.980960  W  Training \"RL\": -- Training run 0 started... \u001b[0m\n",
      "\u001b[93m2024-09-03  14:09:34.980960  W  Training \"RL\": ------------------------------------------------------------------------------ \u001b[0m\n",
      "\u001b[93m2024-09-03  14:09:34.980960  W  Training \"RL\": ------------------------------------------------------------------------------ \n",
      " \u001b[0m\n",
      "\u001b[93m2024-09-03  14:09:35.005245  W  Training \"RL\": ------------------------------------------------------------------------------ \u001b[0m\n",
      "\u001b[93m2024-09-03  14:09:35.005245  W  Training \"RL\": -- Training episode 0 started... \u001b[0m\n",
      "\u001b[93m2024-09-03  14:09:35.005245  W  Training \"RL\": ------------------------------------------------------------------------------ \n",
      " \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yuwono\\.conda\\envs\\workshop\\lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.data to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.data` for environment variables or `env.get_wrapper_attr('data')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\Yuwono\\AppData\\Roaming\\Python\\Python39\\site-packages\\glfw\\__init__.py:912: GLFWError: (65537) b'The GLFW library is not initialized'\n",
      "  warnings.warn(message, GLFWError)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m2024-09-03  14:09:38.543565  W  Training \"RL\": Limit of 1000 cycles per episode reached (Training) \u001b[0m\n",
      "\u001b[93m2024-09-03  14:09:38.543565  W  Training \"RL\": ------------------------------------------------------------------------------ \u001b[0m\n",
      "\u001b[93m2024-09-03  14:09:38.543565  W  Training \"RL\": -- Training episode 0 finished after 1000 cycles \u001b[0m\n",
      "\u001b[93m2024-09-03  14:09:38.543565  W  Training \"RL\": -- Training cycles finished: 1000 \u001b[0m\n",
      "\u001b[93m2024-09-03  14:09:38.543565  W  Training \"RL\": ------------------------------------------------------------------------------ \n",
      "\n",
      " \u001b[0m\n",
      "\u001b[93m2024-09-03  14:09:38.544574  W  Training \"RL\": ------------------------------------------------------------------------------ \u001b[0m\n",
      "\u001b[93m2024-09-03  14:09:38.544574  W  Training \"RL\": -- Training episode 1 started... \u001b[0m\n",
      "\u001b[93m2024-09-03  14:09:38.544574  W  Training \"RL\": ------------------------------------------------------------------------------ \n",
      " \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "cycle_limit             = 30000\n",
    "cycles_per_epi_limit    = 1000\n",
    "logging                 = Log.C_LOG_WE\n",
    "visualize               = True\n",
    "path                    = str(Path.home())\n",
    "plotting                = True\n",
    "\n",
    "training = RLTraining(\n",
    "    p_scenario_cls=Scenario_UR5_Mujoco,\n",
    "    p_cycle_limit=cycle_limit,\n",
    "    p_cycles_per_epi_limit=cycles_per_epi_limit,\n",
    "    p_path=path,\n",
    "    p_visualize=visualize,\n",
    "    p_logging=logging\n",
    ")\n",
    "\n",
    "training.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00742266",
   "metadata": {},
   "source": [
    "### 4. Plotting reward values\n",
    "You can execute the following lines of code to visualize the rewards generated by your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6690157d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mem = training.get_results().ds_rewards\n",
    "data_printing = {mem.names[0]: [False],\n",
    "                 mem.names[1]: [False],\n",
    "                 mem.names[2]: [False],\n",
    "                 mem.names[3]: [False],\n",
    "                 mem.names[4]: [True, 0, -1]}\n",
    "mem_plot = DataPlotting(mem,\n",
    "                        p_showing=plotting,\n",
    "                        p_printing=data_printing,\n",
    "                        p_type=DataPlotting.C_PLOT_TYPE_EP_S,\n",
    "                        p_window=10)\n",
    "mem_plot.get_plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990429a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
