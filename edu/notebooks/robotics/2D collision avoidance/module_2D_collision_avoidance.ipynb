{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "240fa22d",
   "metadata": {},
   "source": [
    "# Workshop 1 - 2D Collision Avoidance Using Reinforcement Learning\n",
    "\n",
    "<b>Project : MLPro - The integrative middleware framework for standardized machine learning</b> <br>\n",
    "<b>Package : [mlpro_int_sb3](https://mlpro-int-sb3.readthedocs.io/en/latest/)</b> <br>\n",
    "<b>Module : [howto_rl_env_003_train_agent_with_sb3_policy_on_2D_collision_avoidance_environment.py](https://github.com/fhswf/MLPro-Int-SB3/blob/main/test/howtos/environment/howto_rl_env_003_train_agent_with_sb3_policy_on_2D_collision_avoidance_environment.py)</b><br>\n",
    "<b>Version : 1.0.0</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd75d78",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Introduction of the Environment\n",
    "\n",
    "In this workshop, we aim to address a basic 2D collision avoidance problem using reinforcement learning. Details about the environment can be found at the following link: [2D Collision Detection Environment](https://mlpro.readthedocs.io/en/latest/content/03_machine_learning/mlpro_rl/sub/env/pool/2Dcollisiondetection.html).\n",
    "\n",
    "Initially, we will work with a setup involving 5 nodes, which include an initial node, a target node, and 2 controllable nodes. The obstacle remains static, while the target node can move among 4 different positions. The action space is constrained within the range of [-0.05, 0.05]. Before proceeding, you should be able to answer the following questions:\n",
    "1. What are the states?\n",
    "2. What is the action?\n",
    "3. What are the objectives of the RL training?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da108150",
   "metadata": {},
   "source": [
    "## Tasks Descriptions\n",
    "1. **Reward Shaping**: Developing a reward function to steer the RL agent towards desired behaviors more effectively by offering intermediate rewards or penalties.\n",
    "2. **Policy Selection**: Choosing from three different RL algorithms, namely PPO, A2C, and DDPG.\n",
    "3. **Run RL Training**: Execute the RL training setup in MLPro and analyze the results. If the goals are not met, consider revisiting and adjusting the previous two tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354400c4",
   "metadata": {},
   "source": [
    "## Executable Code\n",
    "\n",
    "### 0. Import the related modules\n",
    "The task involves importing the necessary libraries and packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6e693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlpro.bf.plot import DataPlotting\n",
    "from mlpro.rl import *\n",
    "from mlpro.rl.pool.envs.collisionavoidance_2D import DynamicTrajectoryPlanner\n",
    "from stable_baselines3 import PPO, DDPG, A2C\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from mlpro_int_sb3.wrappers import WrPolicySB32MLPro\n",
    "from mlpro_int_gymnasium.wrappers import WrEnvMLPro2GYM\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5f0833",
   "metadata": {},
   "source": [
    "### 1. Design your own reward function\n",
    "Your first task is to create a reward function that assigns rewards and penalties to the agent for each iteration. You can design your reward function using the following three components:\n",
    "1. Number of Collide Points (`number_of_collide_points`): This parameter measures the number of collision points with the obstacle.\n",
    "2. Number of Collide Lines (`number_of_collide_lines`): This parameter tracks the number of collision lines with the obstacle.\n",
    "3. Distance of the Trajectory (`distance`): This parameter calculates the distance of the trajectory from the starting node to the target node, including visits to all controllable nodes.\n",
    "\n",
    "Then, the reward function should be stored in a variable named `total_rewards`.\n",
    "\n",
    "Here is an example of a reward function: If there is a collision with lines or points, a penalty of 10 points is given, along with a penalty based on the trajectory distance. Conversely, if there are no collisions, a reward of 10 points is granted. The reward function is designed as follows:\n",
    "```python\n",
    "if number_of_collide_lines != 0 or number_of_collide_points != 0:\n",
    "    total_rewards = -(10+distance)\n",
    "else:\n",
    "    total_rewards = 10\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a22fae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDynamicTrajectoryPlanner(DynamicTrajectoryPlanner):\n",
    "    \n",
    "    def _compute_reward(self, p_state_old:State, p_state_new:State) -> Reward:\n",
    "        \n",
    "        number_of_collide_points = 0\n",
    "        number_of_collide_lines = 0\n",
    "        for _ in self.collide_point_list:\n",
    "            number_of_collide_points += 1\n",
    "        for _ in self.collide_line_list:\n",
    "            number_of_collide_lines += 1\n",
    "\n",
    "        distance = self._calc_distance()\n",
    "        \n",
    "        # Insert your reward function in this section\n",
    "        if number_of_collide_lines != 0 or number_of_collide_points != 0:\n",
    "            total_rewards = -(10+distance)\n",
    "        else:\n",
    "            total_rewards = 10\n",
    "            \n",
    "        reward = Reward()\n",
    "        reward.set_overall_reward(total_rewards)\n",
    "        \n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c117d6bb",
   "metadata": {},
   "source": [
    "### 2. RL policy selection\n",
    "Your second task is to choose the most suitable RL algorithm. We provide three different algorithms from the Stable-Baselines3 (SB3) library, which is integrated with MLPro through a wrapper called MLPro-Int-SB3. The available algorithms are:\n",
    "1. [Advantage Actor-Critic (A2C)](https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html)\n",
    "2. [Proximal Policy Optimization (PPO)](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html)\n",
    "3. [Deep Deterministic Policy Gradient (DDPG)](https://stable-baselines3.readthedocs.io/en/master/modules/ddpg.html)\n",
    "\n",
    "We have provided default implementations for each algorithm, with standard parameter values already set. You can select an algorithm by placing the code for its initialization in the `ScenarioTrajectoryPlanning` Class. Below are the code snippets for each algorithm:\n",
    "1. **A2C**\n",
    "```python\n",
    "policy_kwargs = dict(activation_fn=torch.nn.Tanh,\n",
    "                      net_arch=dict(pi=[128, 128], vf=[128, 128]))\n",
    "policy_sb3 = A2C(\n",
    "    policy=\"MlpPolicy\",\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=100,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    env=None,\n",
    "    _init_setup_model=False,\n",
    "    device=\"cpu\",\n",
    "    seed=2)\n",
    "```\n",
    "2. **PPO**\n",
    "```python\n",
    "policy_kwargs = dict(activation_fn=torch.nn.Tanh,\n",
    "                      net_arch=dict(pi=[128, 128], vf=[128, 128]))\n",
    "policy_sb3 = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=100,\n",
    "    env=None,\n",
    "    _init_setup_model=False,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    device=\"cpu\",\n",
    "    seed=2)\n",
    "```\n",
    "3. **DDPG**\n",
    "```python\n",
    "action_space = WrEnvMLPro2GYM.recognize_space(self._env.get_action_space())\n",
    "n_actions = action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "policy_kwargs = dict(net_arch=dict(pi=[128, 128], qf=[128, 128]))\n",
    "policy_sb3 = DDPG(\n",
    "    policy=\"MlpPolicy\",\n",
    "    learning_rate=3e-4,\n",
    "    buffer_size=1000,\n",
    "    batch_size=128,\n",
    "    learning_starts=1001,\n",
    "    action_noise=action_noise,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    env=None,\n",
    "    _init_setup_model=False,\n",
    "    device=\"cpu\",\n",
    "    seed=3)\n",
    "```\n",
    "\n",
    "In the default version of these executable codes, the DDPG algorithm is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7081593f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScenarioTrajectoryPlanning(RLScenario):\n",
    "    C_NAME = 'Trajectory Planning'\n",
    "\n",
    "    def _setup(self, p_mode, p_ada: bool, p_visualize: bool, p_logging) -> Model:\n",
    "\n",
    "        self._env = MyDynamicTrajectoryPlanner(\n",
    "            p_visualize=p_visualize,\n",
    "            p_logging=p_logging,\n",
    "            p_num_point=4,\n",
    "            p_action_boundaries=[-0.05,0.05],\n",
    "            p_dt=0.01,\n",
    "            p_cycle_limit=500\n",
    "            )\n",
    "        \n",
    "        # Insert the initialization code for your selected algorithm in this section\n",
    "        action_space = WrEnvMLPro2GYM.recognize_space(self._env.get_action_space())\n",
    "        n_actions = action_space.shape[-1]\n",
    "        action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "        policy_kwargs = dict(net_arch=dict(pi=[128, 128], qf=[128, 128]))\n",
    "        policy_sb3 = DDPG(\n",
    "            policy=\"MlpPolicy\",\n",
    "            learning_rate=3e-4,\n",
    "            buffer_size=1000,\n",
    "            batch_size=128,\n",
    "            learning_starts=1001,\n",
    "            action_noise=action_noise,\n",
    "            policy_kwargs=policy_kwargs,\n",
    "            env=None,\n",
    "            _init_setup_model=False,\n",
    "            device=\"cpu\",\n",
    "            seed=3)\n",
    "\n",
    "        policy_wrapped = WrPolicySB32MLPro(\n",
    "            p_sb3_policy=policy_sb3,\n",
    "            p_cycle_limit=self._cycle_limit,\n",
    "            p_observation_space=self._env.get_state_space(),\n",
    "            p_action_space=self._env.get_action_space(),\n",
    "            p_ada=p_ada,\n",
    "            p_visualize=p_visualize,\n",
    "            p_logging=p_logging)\n",
    "\n",
    "        return Agent(\n",
    "            p_policy=policy_wrapped,\n",
    "            p_envmodel=None,\n",
    "            p_name='Smith',\n",
    "            p_ada=p_ada,\n",
    "            p_visualize=p_visualize,\n",
    "            p_logging=p_logging\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83549392",
   "metadata": {},
   "source": [
    "### 3. RL Training configuration\n",
    "Your next task is to initiate the RL training by executing the following code. You have the flexibility to adjust two key variables: `cycle_limit` and `cycles_per_epi_limit`. Currently, these are set to 10000 and 500, respectively. For the initial run, it is advisable to maintain these default values. Note that the environment's cycle limit is also 500, so ensure that the `cycles_per_epi_limit` does not exceed this value. Once the training begins, a plot will appear, allowing you to monitor the training progress in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3552d7d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cycle_limit             = 10000\n",
    "cycles_per_epi_limit    = 500\n",
    "logging                 = Log.C_LOG_WE\n",
    "visualize               = True\n",
    "path                    = str(Path.home())\n",
    "plotting                = True\n",
    "\n",
    "training = RLTraining(\n",
    "    p_scenario_cls=ScenarioTrajectoryPlanning,\n",
    "    p_cycle_limit=cycle_limit,\n",
    "    p_cycles_per_epi_limit=cycles_per_epi_limit,\n",
    "    p_path=path,\n",
    "    p_visualize=visualize,\n",
    "    p_logging=logging\n",
    ")\n",
    "\n",
    "training.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d6788e",
   "metadata": {},
   "source": [
    "### 4. Plotting reward values\n",
    "You can execute the following lines of code to visualize the rewards generated by your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bb15e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = training.get_results().ds_rewards\n",
    "data_printing = {mem.names[0]: [False],\n",
    "                 mem.names[1]: [False],\n",
    "                 mem.names[2]: [False],\n",
    "                 mem.names[3]: [False],\n",
    "                 mem.names[4]: [True, 0, -1]}\n",
    "mem_plot = DataPlotting(mem,\n",
    "                        p_showing=plotting,\n",
    "                        p_printing=data_printing,\n",
    "                        p_type=DataPlotting.C_PLOT_TYPE_EP,\n",
    "                        p_window=100)\n",
    "mem_plot.get_plots()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
